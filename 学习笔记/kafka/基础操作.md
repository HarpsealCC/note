# kafka学习笔记

## 简单介绍
Kafka消息队列是一个开源的分布式事件流平台，最初由LinkedIn开发并贡献给了Apache软件基金会。它旨在处理大规模的实时数据流，具有高吞吐量、低延迟和可持久化等特点。

Kafka的核心概念包括：

1. 消息： Kafka基于发布-订阅模式，消息由生产者发布到主题（Topic）中，然后由消费者订阅主题并处理消息。
2. 主题： 主题是消息的逻辑容器，类似于一个命名的通道或数据流。生产者将消息发布到主题，而消费者订阅主题以接收消息。
3. 分区： 每个主题可以分为一个或多个分区，分区是消息的物理存储单元。分区使得Kafka能够水平扩展，并允许并行处理消息。
4. 生产者： 生产者负责向主题发布消息，可以指定消息的键（Key），以便控制消息在分区之间的分布。
5. 消费者： 消费者订阅一个或多个主题，并从分区中读取消息进行处理。
6. 代理（Broker）： Kafka集群由多个代理组成，每个代理负责存储和管理分区中的消息。生产者和消费者与代理进行通信。

Kafka广泛应用于大数据处理、日志收集、实时分析等场景，例如实时数据处理、日志聚合、指标监控等。其高可用性、可扩展性和容错性使得它成为处理实时数据流的首选解决方案之一。

## 简单使用
### 找到脚本
kafka在linux中提供了丰富的使用脚本，存放在安装目录下的bin文件夹下
不清楚安装路径可以使用`find / -name kafka-topics.sh 2>/dev/null`进行查找

### 发布主题
将load.csv中的数据发布到my-topic中
```
cat load.csv | bin/kafka-console-producer.sh --broker-list localhost:9092 --topic my-topic
```

### 消费主题
从开始消费my-topic中的数据
```
bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic my-topic --from-beginning
```




## 问题
### 如何发布gbk编码的数据到topic

使用脚本
```
from kafka import KafkaProducer
import csv

# Kafka broker地址
bootstrap_servers = '172.16.48.8:19193'
# Kafka主题名称
topic = 'gbk-load-topic'

# 创建KafkaProducer实例
producer = KafkaProducer(bootstrap_servers=bootstrap_servers)

# 读取CSV文件并发送到Kafka主题
with open('gbk_common_chinese_data.csv', 'r', encoding='gbk') as csvfile:
    csvreader = csv.reader(csvfile)
    for row in csvreader:
        # 将CSV行转换为字节串，并发送到Kafka主题
        producer.send(topic, ','.join(row).encode('gbk'))

# 关闭KafkaProducer连接
producer.close()
```